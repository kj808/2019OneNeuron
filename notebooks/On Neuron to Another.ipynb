{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Neuron to Another\n",
    "\n",
    "* Highly based on https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6\n",
    "\n",
    "### Introduction\n",
    "In this notebook, I created a simple, three layer neural network (input, hidden, output). This was based off of the link provided above. Due to the activation function used and setup, the NN can accomplish binary classification problems. With this in mind, I chose two datasets: one with multiclass output but it altered to focus on only one of the classes as well as a pure binary classification set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "This neural network is comprised of three layers (input,hidden,output). The hidden layer is fully connected and the number of nodes is determined by the user as the data depends on how many should be selected. In order to learn these weights between each of the layers, feed forward and back propagation is leveraged. Feed forward updates the output based on the weights already calculated. On the first instance, the weights are randomized as we do not yet know anything about the data aside from the input and output vectors that were passed. \n",
    "\n",
    "These randomized weights are used for the feed forward algorithm which leverages sigmoid activation function of the dot product of the input layer to the weights and that result to the second layer of weights which result in the output. \n",
    "\n",
    "Now we leverage back propagation to evaluate the difference between our expected values(output layer) opposed to our calculated layer(self.output). This helps readjust the weights to better approximate the expected value.\n",
    "\n",
    "The first iteration of feed forward and backward propagation does not fully learn the input, thus multiple iterations are required in order to learn the data. Furthermore, depending on the amount of data, it may be highly possible to overtrain or undertrain. A specifed learning rate can be passed in order to achieve the desired result. There are learning rates that are adaptive, but for the extent of this project, I decided to chose a value based on what I saw in the NN output accuracy.\n",
    "\n",
    "Lastly, I added another function that can be used to evaluate the trained neural network. This requires new input that the user wants to classify based on the previous weights. This only involves feed forward with the passed data once and should not update the existing weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define activation functions\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+ np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "#Define neural network\n",
    "class NeuralNetwork:\n",
    "    def __init__(self,x,y,nodeNumbers,learningrate):\n",
    "        self.input      = x\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],nodeNumbers) \n",
    "        self.weights2   = np.random.rand(nodeNumbers,1)                 \n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(self.y.shape)\n",
    "        self.learningRate= learningrate\n",
    "\n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n",
    "    \n",
    "    def backprop(self):\n",
    "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
    "        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * (self.learningRate*sigmoid_derivative(self.output))))\n",
    "        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * (self.learningRate*sigmoid_derivative(self.output)), self.weights2.T) * (self.learningRate*sigmoid_derivative(self.layer1))))\n",
    "\n",
    "        # update the weights with the derivative (slope) of the loss function\n",
    "        self.weights1 += d_weights1\n",
    "        self.weights2 += d_weights2\n",
    "    \n",
    "    def evalNN(self, newInput):\n",
    "        layer1 = sigmoid(np.dot(newInput, self.weights1))\n",
    "        output2 = sigmoid(np.dot(layer1, self.weights2))\n",
    "        return output2\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing DataFrame\n",
    "In order to train a neural network on randomized data, I need to shuffle then pass the respective X and Y values to parse into a training, testing and validation set. With the assumption of X and Y should be shuffled prior, we can simply break the DataFrame based on indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parsing frames into training, testing, and validation\n",
    "def parser(X,Y):\n",
    "    #split data into training 80 and testing 10 and validation 10\n",
    "    #since we always shuffle, we can pick consequtively instead of randomly sampling on a random shuffle\n",
    "    trainingSetIndex= int(0.80*len(Y))#first 80%\n",
    "    testingSetIndex= int(0.10*len(Y)+trainingSetIndex) #next 10%\n",
    "    validationSetIndex= int(0.10*len(Y)+testingSetIndex)#last 10%\n",
    "    print(\"Training Index: \" + str(trainingSetIndex))\n",
    "    print(\"Testing Index: \" + str(testingSetIndex))\n",
    "    print(\"Validation Index: \" + str(validationSetIndex))\n",
    "\n",
    "    #Split dataset\n",
    "    training=X.iloc[0:trainingSetIndex,:]\n",
    "    trainingY=Y[0:trainingSetIndex]\n",
    "    testing=X.iloc[trainingSetIndex:testingSetIndex,:]\n",
    "    testingY=Y[trainingSetIndex:testingSetIndex]\n",
    "    validation=X.iloc[testingSetIndex:len(X),:]\n",
    "    validationY=Y[testingSetIndex:len(X)]\n",
    "    print(\"-------------------------\")\n",
    "    print(\"Training: \")\n",
    "    print(training[:5])\n",
    "    print(trainingY[:5])\n",
    "    print(\"-------------------------\")\n",
    "    print(\"Testing: \")\n",
    "    print(testing[:5])\n",
    "    print(testingY[:5])\n",
    "    print(\"-------------------------\")\n",
    "    print(\"Validation: \")\n",
    "    print(validation[:5])\n",
    "    print(validationY[:5])\n",
    "    \n",
    "    return training,trainingY,testing,testingY,validation,validationY\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Accuracy\n",
    "In order to understand the validity of the neural network, I created a function that compares the NN output and the expected values and calculate the percentage of the correctly guessed values. Due to the NN outputting numbers in type double, I must round and calculate appropriately as zero means one class and 1 means the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check accuracy of neural network\n",
    "def checkAccuracy(NNoutput,Y):\n",
    "    #round output vector\n",
    "    checker=np.round(NNoutput)\n",
    "    #print(checker[:5])\n",
    "    #substract output vector with actual values\n",
    "    checker=np.subtract(checker,Y)\n",
    "    #count how many values are not 0\n",
    "    count=np.count_nonzero(checker)\n",
    "    #convert to percentage guessed correctly\n",
    "    percentage=((len(Y)-count)/len(Y))*100\n",
    "    return percentage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSet 1 : Iris Dataset\n",
    "Iris dataset involves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm          Species\n",
      "0            6.6           2.9            4.6           1.3  Iris-versicolor\n",
      "1            6.1           2.8            4.0           1.3  Iris-versicolor\n",
      "2            6.3           2.3            4.4           1.3  Iris-versicolor\n",
      "3            4.6           3.1            1.5           0.2      Iris-setosa\n",
      "4            5.0           2.0            3.5           1.0  Iris-versicolor\n",
      "['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "#Read in dataset 1\n",
    "#Set inputs to\n",
    "iris=pd.read_csv('../data/external/iris-species/Iris.csv')\n",
    "iris=iris.drop(columns=['Id'])\n",
    "iris=iris.dropna()\n",
    "irisnew=iris.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(irisnew.head())\n",
    "print(iris.Species.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PetalLengthCm  PetalWidthCm\n",
      "0            4.6           1.3\n",
      "1            4.0           1.3\n",
      "2            4.4           1.3\n",
      "3            1.5           0.2\n",
      "4            3.5           1.0\n",
      "Output\n",
      "0    Iris-versicolor\n",
      "1    Iris-versicolor\n",
      "2    Iris-versicolor\n",
      "3        Iris-setosa\n",
      "4    Iris-versicolor\n",
      "Name: Species, dtype: object\n",
      "Converted Output\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "Output Mapping\n",
      "{'Iris-setosa': 0, 'Iris-versicolor': 0, 'Iris-virginica': 1}\n"
     ]
    }
   ],
   "source": [
    "#Distribute lengths and widths to one-hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "#X = irisnew.iloc[:,[0,1,2,3]]\n",
    "X = irisnew.iloc[:,[2,3]]\n",
    "print(X.head())\n",
    "enc.fit(X)\n",
    "\n",
    "#Output the species to a mapping \n",
    "outputDictionary={}\n",
    "for index,entry in enumerate(iris.Species.unique()):\n",
    "    if entry=='Iris-virginica':\n",
    "        outputDictionary[entry]=1\n",
    "    else:\n",
    "        outputDictionary[entry]=0\n",
    "Y0=irisnew.iloc[:,4]\n",
    "print(\"Output\")\n",
    "print(Y0[:5])\n",
    "Y=[]\n",
    "for value in Y0:\n",
    "    if value=='Iris-virginica':\n",
    "        Y.append([1])\n",
    "    else:\n",
    "        Y.append([0])\n",
    "print(\"Converted Output\")\n",
    "Y=np.asarray(Y)\n",
    "print(Y[:5])\n",
    "\n",
    "print(\"Output Mapping\")\n",
    "print(outputDictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Index: 120\n",
      "Testing Index: 135\n",
      "Validation Index: 150\n",
      "-------------------------\n",
      "Training: \n",
      "   PetalLengthCm  PetalWidthCm\n",
      "0            4.6           1.3\n",
      "1            4.0           1.3\n",
      "2            4.4           1.3\n",
      "3            1.5           0.2\n",
      "4            3.5           1.0\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "-------------------------\n",
      "Testing: \n",
      "     PetalLengthCm  PetalWidthCm\n",
      "120            1.5           0.1\n",
      "121            4.8           1.8\n",
      "122            1.4           0.3\n",
      "123            5.1           1.5\n",
      "124            4.4           1.4\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "-------------------------\n",
      "Validation: \n",
      "     PetalLengthCm  PetalWidthCm\n",
      "135            4.4           1.2\n",
      "136            4.3           1.3\n",
      "137            5.6           2.1\n",
      "138            6.7           2.2\n",
      "139            4.1           1.3\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "#split data into training 80 and testing 10 and validation 10\n",
    "#since we always shuffle, we can pick consequtively instead of randomly sampling on a random shuffle\n",
    "training,trainingY,testing,testingY,validation,validationY=parser(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2058228 ]\n",
      " [0.25998109]\n",
      " [0.2229524 ]\n",
      " [0.01250293]\n",
      " [0.09207396]]\n",
      "Training: 95.83333333333334%\n",
      "Testing: 86.66666666666667%\n",
      "Validation: 100.0%\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(training,trainingY,4,0.025)\n",
    "\n",
    "iterations=1200\n",
    "for i in range(iterations):\n",
    "    nn.feedforward()\n",
    "    nn.backprop()\n",
    "    \n",
    "#Print neural network output\n",
    "print(nn.output[:5])\n",
    "#Compare actual output to learning\n",
    "print(\"Training: \"+str(checkAccuracy(nn.output,trainingY))+\"%\")\n",
    "\n",
    "#Compare with testing\n",
    "output=nn.evalNN(testing)\n",
    "print(\"Testing: \"+str(checkAccuracy(output, testingY))+\"%\")\n",
    "\n",
    "#Compare with validation\n",
    "output=nn.evalNN(validation)\n",
    "print(\"Validation: \"+str(checkAccuracy(output, validationY))+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2 : Statlog German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: \n",
      "(1000, 21)\n",
      "-----------------------------------\n",
      "DataFrame:\n",
      "  ExistingChecking  MonthDuration CreditHist Purpose  CreditAmt SavingsAcct  \\\n",
      "0              A14             30        A34     A43       5954         A61   \n",
      "1              A11             12        A32     A43       1680         A63   \n",
      "2              A14             24        A34     A41       2346         A61   \n",
      "3              A14             21        A34     A42       2288         A61   \n",
      "4              A14              6        A34     A40       6761         A61   \n",
      "\n",
      "  PresentEmploy  InstallmentRate PersonalStatus Other  ...   Property Age  \\\n",
      "0           A74                3            A93  A102  ...       A123  38   \n",
      "1           A75                3            A94  A101  ...       A121  35   \n",
      "2           A74                4            A93  A101  ...       A123  35   \n",
      "3           A72                4            A92  A101  ...       A122  23   \n",
      "4           A74                1            A93  A101  ...       A124  45   \n",
      "\n",
      "   OtherInstallPlan Housing ExistingCredits   Job PeopleLiable  Telephone  \\\n",
      "0              A143    A152               1  A173            1       A191   \n",
      "1              A143    A152               1  A173            1       A191   \n",
      "2              A143    A152               2  A173            1       A192   \n",
      "3              A143    A152               1  A173            1       A192   \n",
      "4              A143    A152               2  A174            2       A192   \n",
      "\n",
      "  Foreign label  \n",
      "0    A201     0  \n",
      "1    A201     0  \n",
      "2    A201     0  \n",
      "3    A201     0  \n",
      "4    A201     0  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "#Dataset 2\n",
    "#Source: http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/\n",
    "credit=pd.read_csv('../data/external/german/german.data',header=None,sep=' ')\n",
    "credit.columns=[\"ExistingChecking\",\"MonthDuration\",\"CreditHist\",\"Purpose\",\"CreditAmt\",\"SavingsAcct\",\"PresentEmploy\",\"InstallmentRate\",\"PersonalStatus\",\"Other\",\"PresentResidence\",\"Property\",\"Age\",\"OtherInstallPlan\",\"Housing\",\"ExistingCredits\",\"Job\",\"PeopleLiable\",\"Telephone\",\"Foreign\",\"label\"]\n",
    "\n",
    "credit=credit.dropna() #remove nulls\n",
    "credit=credit.replace({'label': {1: 4, 2: 5}}) #set labels between 0 and 1\n",
    "credit=credit.replace({'label': {4: 0, 5: 1}})\n",
    "creditnew=credit.sample(frac=1).reset_index(drop=True) #shuffle\n",
    "\n",
    "\n",
    "print(\"Shape: \")\n",
    "print(creditnew.shape)\n",
    "print(\"-----------------------------------\")\n",
    "print(\"DataFrame:\")\n",
    "print(creditnew.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Distribute lengths and widths to one-hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "X = creditnew\n",
    "enc.fit(X)\n",
    "\n",
    "#Set Y values\n",
    "\n",
    "Y=creditnew.iloc[:,-1]\n",
    "print(\"Output\")\n",
    "print(Y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.20783627]\n",
      " [0.26768125]\n",
      " [0.22683163]\n",
      " [0.01457052]\n",
      " [0.09738911]]\n",
      "Training: 95.83333333333334%\n",
      "Testing: 86.66666666666667%\n",
      "Validation: 100.0%\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(training,trainingY,4,0.025)\n",
    "\n",
    "iterations=1200\n",
    "for i in range(iterations):\n",
    "    nn.feedforward()\n",
    "    nn.backprop()\n",
    "    \n",
    "#Print neural network output\n",
    "print(nn.output[:5])\n",
    "#Compare actual output to learning\n",
    "print(\"Training: \"+str(checkAccuracy(nn.output,trainingY))+\"%\")\n",
    "\n",
    "#Compare with testing\n",
    "output=nn.evalNN(testing)\n",
    "print(\"Testing: \"+str(checkAccuracy(output, testingY))+\"%\")\n",
    "\n",
    "#Compare with validation\n",
    "output=nn.evalNN(validation)\n",
    "print(\"Validation: \"+str(checkAccuracy(output, validationY))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
